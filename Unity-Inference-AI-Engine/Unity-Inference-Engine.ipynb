{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d1969f",
   "metadata": {},
   "source": [
    "# **[Unity Inference Engine](https://docs.unity3d.com/Packages/com.unity.ai.inference@2.3/manual/get-started.html)** \n",
    "\n",
    "**Inference Engine** is the neural network inference library integrated into Unity.\n",
    "It allows you to **import pre-trained models** (especially in **ONNX** format) and run them in real time directly on the user's device (CPU or GPU), without the need for external servers.\n",
    "\n",
    "#### Compatibility\n",
    "\n",
    "* Works on **all Unity runtime platforms** (PC, console, mobile, XR).\n",
    "* Performance depends on model complexity, hardware, and engine type chosen.\n",
    "\n",
    "#### Supported Models\n",
    "\n",
    "* ONNX format with opset v7–15.\n",
    "* Wide compatibility with pre-trained models available from: Hugging Face, Kaggle Models, PyTorch Hub, Model Zoo, Meta Research, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2d1b0",
   "metadata": {},
   "source": [
    "**ONNX** stands for **Open Neural Network Exchange**.\n",
    "It is an **open standard format** created by Microsoft and Facebook (now supported by many companies) for saving and exchanging machine learning and deep learning models independently of the framework they were trained in.\n",
    "\n",
    "In practice:\n",
    "\n",
    "* You can **train a model** in **PyTorch, TensorFlow, Keras**, or other frameworks.\n",
    "* You then **export it to ONNX**.\n",
    "* Once in ONNX format, the model can be **imported and run in other platforms** (e.g., Unity Sentis, ONNX Runtime, Hugging Face, etc.).\n",
    "\n",
    "### Why is it useful?\n",
    "\n",
    "* **Portability** → you are not tied to a single framework.\n",
    "* **Compatibility** → the same file works in multiple environments.\n",
    "* **Optimization** → Many libraries (e.g., ONNX Runtime, TensorRT) optimize models to run faster on CPU/GPU.\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "If you train a neural network in **PyTorch** that recognizes images, you can export it to `.onnx`. Then, without having to rewrite the model, you can use it in Unity with **Sentis**, or integrate it into a C++, Python, or mobile application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04876a9",
   "metadata": {},
   "source": [
    "### Installing Inference Engine in Unity\n",
    "\n",
    "**Compatibility:**\n",
    "\n",
    "* **Inference Engine** works with **Unity 6** and later.\n",
    "\n",
    "**Installation procedure:**\n",
    "\n",
    "1. Create a new Unity project or open an existing one.\n",
    "2. Open the **Package Manager**: `Window > Package Manager`.\n",
    "3. Click the **+** button and choose **Add package by name...**\n",
    "4. Enter:\n",
    "\n",
    "```\n",
    "com.unity.ai.inference\n",
    "```\n",
    "5. Press **Add** → the package will be installed in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71082bbe",
   "metadata": {},
   "source": [
    "## Sentis / Inference Engine Workflow\n",
    "\n",
    "1. **Import the namespace**\n",
    "\n",
    "```csharp\n",
    "using UnityEngine;\n",
    "using Unity.InferenceEngine; // in newer versions, this may be Unity.AI.Inference\n",
    "```\n",
    "\n",
    "2. **Load the ONNX model**\n",
    "\n",
    "* Place the `.onnx` file in your project's `Assets/Resources/` folder.\n",
    "* Then in code:\n",
    "\n",
    "```csharp\n",
    "ModelAsset modelAsset = Resources.Load(\"model-name\") as ModelAsset;\n",
    "var runtimeModel = ModelLoader.Load(modelAsset);\n",
    "```\n",
    "\n",
    "3. **Create an input (Tensor)**\n",
    "You can convert textures or arrays:\n",
    "\n",
    "```csharp\n",
    "// From Texture2D\n",
    "Texture2D inputTexture = Resources.Load(\"image\") as Texture2D;\n",
    "Tensor<float> inputTensor = TextureConverter.ToTensor(inputTexture);\n",
    "\n",
    "// From array\n",
    "int[] array = new int[] {1,2,3,4};\n",
    "Tensor<int> arrayTensor = new Tensor<int>(new TensorShape(4), array);\n",
    "```\n",
    "\n",
    "4. **Create the Worker (inference engine)**\n",
    "\n",
    "```csharp\n",
    "Worker worker = new Worker(runtimeModel, BackendType.GPUCompute);\n",
    "// BackendType can be GPUCompute, CPU, etc.\n",
    "```\n",
    "\n",
    "5. **Run the model**\n",
    "\n",
    "```csharp\n",
    "worker.Schedule(inputTensor);\n",
    "```\n",
    "\n",
    "(NB: it's asynchronous → the layers run in the background.)\n",
    "\n",
    "6. **Get the output**\n",
    "\n",
    "```csharp\n",
    "Tensor<float> outputTensor = worker.PeekOutput() as Tensor<float>;\n",
    "Debug.Log(\"Output tensor shape: \" + outputTensor.shape);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Release Notes\n",
    "\n",
    "* In Unity 6.2, the package appears as **Inference Engine (com.unity.ai.inference)**.\n",
    "* In scripts, however, you can still use `using Unity.Sentis;` (some examples remain this way), or `using Unity.AI.Inference;` depending on the release.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
