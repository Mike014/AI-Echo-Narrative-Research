{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc23916e",
   "metadata": {},
   "source": [
    "# **[Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/pdf/2304.03442)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10e070",
   "metadata": {},
   "source": [
    "The paper **introduces generative agents**, virtual characters that **simulate believable human behavior**. They are not simple chatbots: they live in a simulated world, wake up, talk, work, remember experiences, and plan for the future.\n",
    "\n",
    "**The architecture is based on a LLM** enriched with three functions:\n",
    "\n",
    "1. **Memory** (recording experiences),\n",
    "2. **Reflection** (abstracting memories),\n",
    "3. **Planning** (using memories to act).\n",
    "\n",
    "In a Sims-style experiment with 25 agents, from a single input (\"I want to organize a Valentine's Day party\"), the agents spread the news, invited others, made appointments, and participated together.\n",
    "\n",
    "### **Result**:\n",
    "The agents exhibit realistic and emergent behaviors, useful for games, immersive environments, and social research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1f86e",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The authors situate their work within the literature on human-AI interaction and the historic but complex efforts to create agents that credibly simulate human behavior.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "* **Human-AI Interaction**: For decades, efforts have been made to enable users to influence and train models.\n",
    "* **Historical examples**: Crayons (interactive ML), SHRDLU, ELIZA (first natural language interactions).\n",
    "* **Evolution**: systems based on examples/demonstrations, up to prompt authoring and deep learning.\n",
    "* **Agents and Natural Language**: Today, the technology is mature enough for autonomous agents that communicate in natural language in complex online social environments.\n",
    "* **Opportunities**: Natural language becomes a means to enhance human skills in various domains (photo editing, code editing).\n",
    "\n",
    "This line of research reopens classic **HCI questions** (cognitive models, prototyping tools, ubiquitous computing), showing that with LLMs, when integrated with the right architecture, it is possible to build agents that act as credible proxies of human behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bcea4",
   "metadata": {},
   "source": [
    "### Believability as a Goal\n",
    "\n",
    "* “**Believability**” has been a central goal in the development of artificial agents for decades.\n",
    "* The idea is that agents appear to have a life of their own, like Disney characters or NPCs in games, creating the illusion of autonomous decisions and emergent social interactions.\n",
    "\n",
    "### Historical Approaches\n",
    "\n",
    "1. **Rule-based** (finite-state machines, behavior trees):\n",
    "\n",
    "* Dominant in video games (*Mass Effect*, *The Sims*).\n",
    "* They work, but require manual scripting and don't cover all possible interactions.\n",
    "* They don't generate new behaviors beyond the encoded ones.\n",
    "\n",
    "2. **Learning-based (Reinforcement Learning)**:\n",
    "\n",
    "* Huge successes in competitive games (*AlphaStar*, *OpenAI Five*).\n",
    "* However, they only work with clear rewards → not suitable for open worlds with complex social dynamics.\n",
    "\n",
    "3. **Cognitive architectures (SOAR, ACT-R, ICARUS)**:\n",
    "\n",
    "* They integrate short-term/long-term memory and perception-planning-action cycles.\n",
    "* Examples: Quakebot-SOAR, TacAir-SOAR.\n",
    "* Robust for their time, but still based on handwritten procedural knowledge → no real ability to invent new behaviors.\n",
    "\n",
    "### Current status\n",
    "\n",
    "* The problem of creating **truly believable agents** remains open.\n",
    "* Many researchers are satisfied with \"good enough\" solutions for traditional gameplay.\n",
    "* The authors argue, however, that **Large Language Models** now offer the possibility of reopening the challenge, if integrated with an architecture capable of managing **memories and reflections**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded12efc",
   "metadata": {},
   "source": [
    "### Believability as a Goal\n",
    "\n",
    "* “**Believability**” (credibility) has been a central goal in the development of artificial agents for decades.\n",
    "* The idea is that agents appear to have a life of their own, like Disney characters or NPCs in games, creating the illusion of autonomous decisions and emergent social interactions.\n",
    "\n",
    "### Historical Approaches\n",
    "\n",
    "1. **Rule-based** (finite-state machines, behavior trees):\n",
    "\n",
    "* Dominant in video games (*Mass Effect*, *The Sims*).\n",
    "* They work, but require manual scripting and do not cover all possible interactions.\n",
    "* They do not generate new behaviors beyond those encoded.\n",
    "\n",
    "2. **Learning-based (Reinforcement Learning)**:\n",
    "\n",
    "* Huge successes in competitive games (*AlphaStar*, *OpenAI Five*).\n",
    "* However, they only work with clear rewards → not suitable for open worlds with complex social dynamics.\n",
    "\n",
    "3. **Cognitive architectures (SOAR, ACT-R, ICARUS)**:\n",
    "\n",
    "* They integrate short-term/long-term memory and perception-planning-action cycles.\n",
    "* Examples: Quakebot-SOAR, TacAir-SOAR.\n",
    "* Robust for their time, but still based on handwritten procedural knowledge → no real ability to invent new behaviors.\n",
    "\n",
    "### Current status\n",
    "\n",
    "* The problem of creating **truly believable agents** remains open.\n",
    "* Many researchers are satisfied with \"good enough\" solutions for traditional gameplay.\n",
    "* The authors argue, however, that **Large Language Models** now offer the possibility of reopening the challenge, if integrated with an architecture capable of managing **memories and reflections**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b5517",
   "metadata": {},
   "source": [
    "### LLMs as a basis for behavior\n",
    "\n",
    "* Large Language Models (LLMs) capture a wide range of human behaviors from their training data.\n",
    "* With a defined context, they can generate believable responses and simulate realistic behaviors.\n",
    "\n",
    "### Previous Applications\n",
    "\n",
    "* Social simulacra: Fictitious agents to test social dynamics in new systems.\n",
    "* Replication of social and political studies: questionnaires, surveys, synthetic data synthesis.\n",
    "* Video games: interactive fiction, text adventures.\n",
    "* Robotics: decomposition of complex tasks (e.g., “pick up the bottle” → move to the table, pick up the bottle).\n",
    "\n",
    "### Current Limitations\n",
    "\n",
    "* Most research uses first-level prompts (few-shot, chain-of-thought).\n",
    "* These only work well in the **immediate context**, but they don't handle:\n",
    "\n",
    "* **past experiences**\n",
    "* **long-term consistency**\n",
    "* Problem: The models' **limited context window** makes it impossible to integrate all past experiences.\n",
    "\n",
    "### New approach of the paper\n",
    "\n",
    "* The authors go further, proposing an architecture that:\n",
    "\n",
    "* **dynamically integrates past experiences** with the current context,\n",
    "* updates them at every step,\n",
    "* uses them to generate behaviors that can be reinforced or conflicted by new situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd66105",
   "metadata": {},
   "source": [
    "### Smallville: The Sandbox World\n",
    "\n",
    "The authors have created a small world called **Smallville**, similar to *The Sims*, populated by **25 unique agents** represented by sprites.\n",
    "Each agent has an **initial profile in natural language** (occupation, family, relationships) that becomes the basis of their memory.\n",
    "\n",
    "### Agent Behavior\n",
    "\n",
    "* Agents **act and speak in natural language**.\n",
    "* At each step, they produce a sentence describing what they are doing (\"Isabella writes in her diary,\" \"checks email\"), which is then translated into emojis above the avatar.\n",
    "* They can decide whether to ignore each other or start a conversation with other nearby agents.\n",
    "\n",
    "* Example: political discussions between Isabella Rodriguez and Tom Moreno about the elections.\n",
    "\n",
    "### User Interaction\n",
    "\n",
    "* The user communicates with the agents in natural language, assuming a **role or persona** (e.g., a reporter asking questions).\n",
    "* It's also possible to become the agent's **\"inner voice\"** to directly influence their decisions.\n",
    "\n",
    "* Example: By telling John, \"You're running against Sam,\" John actually starts running as a candidate and tells his family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a123277",
   "metadata": {},
   "source": [
    "In **Smallville**, agents live in a village with houses, shops, a school, and interactive objects (beds, stoves, refrigerators, etc.). They move through the space, enter buildings, and react to changes: if a user sets the stove to be on fire, the agent puts it out. The user can also embody an agent and be treated like a real resident.\n",
    "\n",
    "Agents plan their days and interact naturally. **Example**: John Lin wakes up, eats breakfast, talks to his son Eddy about his studies, then to his wife Mei, relaying the conversation. Finally, he opens the pharmacy, while Mei goes to teach. This shows that generative agents **memorize, share information, and create believable daily routines**, giving rise to realistic social dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e4bd9",
   "metadata": {},
   "source": [
    "### 1. Dissemination of Information\n",
    "\n",
    "Officers share news through dialogue.\n",
    "\n",
    "* Example: Sam announces his candidacy for mayor to Tom. The word gradually spreads, until it becomes a topic of discussion throughout the city.\n",
    "\n",
    "### 2. Relationships and Memory\n",
    "\n",
    "Officers form new relationships and recall past interactions.\n",
    "\n",
    "* Example: Sam meets Latoya for the first time in the park. In a subsequent meeting, he recalls her photography project and asks her for updates.\n",
    "\n",
    "### 3. Coordination\n",
    "\n",
    "Officers independently organize joint events.\n",
    "\n",
    "* Example: Isabella initially plans to throw a Valentine's Day party. From there, she invites friends, sets up the venue, and involves Maria (who in turn invites Klaus, her crush). Eventually, more officers show up and celebrate together.\n",
    "\n",
    "---\n",
    "\n",
    "These cases show that generative agents can:\n",
    "\n",
    "* disseminate ideas,\n",
    "* maintain coherent relationships,\n",
    "* coordinate collective activities,\n",
    "without the need for manual programming.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56980e9",
   "metadata": {},
   "source": [
    "### Generative Agent Architecture\n",
    "\n",
    "1. **Perception**\n",
    "\n",
    "* Agents observe the environment (actions, events, interactions).\n",
    "* Each perception is stored in a **memory stream**, a continuous record of experiences in natural language.\n",
    "\n",
    "2. **Memory Retrieval**\n",
    "\n",
    "* When they need to act, they retrieve the most **relevant, recent, and important** memories from the memory stream.\n",
    "\n",
    "3. **Action**\n",
    "\n",
    "* They use those memories to decide what to do in the moment (e.g., talk to someone, react to an event).\n",
    "\n",
    "4. **Reflection**\n",
    "\n",
    "* Over time, they synthesize memories into higher-level **reflections** (e.g., “Sam is interested in politics”).\n",
    "\n",
    "5. **Planning**\n",
    "\n",
    "* From reflections and experiences, they formulate **long-term plans** (e.g., preparing a party, pursuing a relationship).\n",
    "\n",
    "6. **Continuous Loop**\n",
    "\n",
    "* Actions, reflections, and plans feed back into the memory stream, influencing future behaviors.\n",
    "\n",
    "---\n",
    "\n",
    "## Goal and Problem\n",
    "\n",
    "The architecture must produce time-consistent behavior in an open world, integrating:\n",
    "\n",
    "* Perception of the environment (current state),\n",
    "* Past experience (memory),\n",
    "* Reasoning (reflections and plans).\n",
    "\n",
    "An LLM alone can generate plausible \"instantaneous\" actions, but fails to recall relevant past experiences, make important inferences, and maintain long-term consistency (context window limit, fragile planning).\n",
    "\n",
    "## Cornerstone: the *Memory Stream*\n",
    "\n",
    "It is a chronological natural language database that stores everything: observations, reflections, and plans. Each record has its text, creation timestamp, and last access timestamp. The LLM is not conditioned by the entire memory (which is too large), but by a selected subset via retrieval.\n",
    "\n",
    "### Retrieval: How to Select the Right Memories\n",
    "\n",
    "A score is calculated by combining three signals (normalized to \\[0,1] and weighted α=1**):\n",
    "\n",
    "* **Recency**: Priority to what has happened/been recently (exponential decay with a factor of **0.995** for “hours of play”).\n",
    "* **Importance**: Distinguish between trivial and salient events. The LLM assigns a **1–10** rating to poignancy when the memory arises (e.g., “asking a crush out” \\~8).\n",
    "* **Relevance**: Relevance to the current **query context** calculated as **cosine similarity** between the memory embedding and the query embedding.\n",
    "\n",
    "The **top-k** memories, as long as there is space in the model window, fit into the prompt: thus, the action produced is just enough informed without being “drowning” in noise.\n",
    "\n",
    "## Reflection: From Data to Inference\n",
    "\n",
    "Only \"raw\" memory produces shortsighted choices (e.g., choosing the most frequent friend, not the closest one). Therefore, the authors introduce **reflections**: abstract and enduring thoughts, generated **periodically** when the sum of recent importances exceeds a **threshold (150)**.\n",
    "Procedure:\n",
    "\n",
    "1. With the last ~100 memories, the LLM asks **high-level questions** (“What is Klaus passionate about?”).\n",
    "2. For each question, it performs targeted retrieval and **extracts insights** by citing the evidence-based memories.\n",
    "3. The reflections **accumulate in a tree** (leaves = observations; internal nodes = increasingly abstract concepts), reentering the memory stream and becoming recallable.\n",
    "\n",
    "Effect: the agent **generalizes** and maintains **identity themes** (interests, relationships), improving choices and dialogues.\n",
    "\n",
    "## Planning & Reacting: Consistency over Time\n",
    "\n",
    "To avoid \"temporal myopia\" (plausible but repetitive actions), agents:\n",
    "\n",
    "* **Plan from the top down**: first, a **sketch** of the day (5–8 blocks), then breakdown into **time slots**, then into **5–15 minute chunks** (materials, breaks, cleanup).\n",
    "* **Adapt the plan** (*reacting*) to each *tick*: they perceive new observations, decide whether to **continue** or **react** (e.g., seeing the child in the garden → starting a relevant dialogue), **regenerating** the plan from the moment of the deviation.\n",
    "* **Dialogue**: each line is influenced by **synthesized memory** about the interlocutor + **history of the dialogue**, maintaining **relational coherence** (roles, affects, ongoing tasks).\n",
    "\n",
    "**Operational summary**: cycle: perceive → retrieve (R/R/I) → act/speak → reflect → plan/re-plan → record everything in the memory stream. This loop creates **narrative stability** and credible **social emergency**.\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92373332",
   "metadata": {},
   "source": [
    "### **LOGIC RECAP**\n",
    "\n",
    "1. **Pure” LLM**\n",
    "\n",
    "* It doesn't do anything on its own: it generates text given a prompt.\n",
    "* To \"do things,\" you need an **orchestrator** that reads the output and calls tools/environment (code, API, game engine).\n",
    "\n",
    "2. **Memory stream ≠ prompt context**\n",
    "\n",
    "* **Memory stream** (in the paper): a **persistent** archive of experiences (observations, reflections, plans).\n",
    "* **Context**: the **subset** of memories retrieved from the stream and inserted into the prompt **at that moment** (via retrieval: *recency*, *importance*, *relevance*).\n",
    "* So: the stream is \"everything that happened\"; the context is \"what I pass to the LLM now.\"\n",
    "\n",
    "3. **ReAct** (Reason + Act)\n",
    "\n",
    "* It's a **pattern** that alternates **reasoning** (textual trace) and **actions** (tool use) in a loop.\n",
    "* The paper doesn't explicitly call it “ReAct,” but the cycle is **ReAct-like**:\n",
    "**Perceive → Retrieve memories → Reflect/Plan → Propose action (NL) → Engine executes → New observations → Updated memory**.\n",
    "\n",
    "4. **What does the paper add beyond ReAct**\n",
    "\n",
    "* **Reflections** (tree abstractions) to stabilize **identities/interests**.\n",
    "* **Hierarchical planning** (day → hours → 5–15 min) + reactive **re-planning**.\n",
    "* **Grounding** NL ↔ world (tree of areas/objects) and **server** that applies the effects (pathfinding, states).\n",
    "\n",
    "In short:\n",
    "\n",
    "**Agent = LLM + Memory Stream (persistent) + Retrieval → [Context] + Reflection + Planning + Tools/Env + Orchestrator (loop).**\n",
    "\n",
    "LLM **with** memory “stream → context” and a **ReAct**-style loop can “reason” (in the sense of planning/justifying) **and** “act” (via tools/environment). Without these pieces, all that remains is text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
