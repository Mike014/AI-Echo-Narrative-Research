{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe811da",
   "metadata": {},
   "source": [
    "# [Unity ML-Agents — Quick Guide](https://docs.unity3d.com/Packages/com.unity.ml-agents%404.0/manual/ML-Agents-Overview.html)\n",
    "\n",
    "> **Objective**: Give you a clear, actionable summary of what ML‑Agents is, how it works (essential theory), how to install it **today**, and how to run the **3D Balance Ball sample** end‑to‑end up to the ONNX model. This is a **first draft** to review together.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) What is ML‑Agents, in two sentences\n",
    "\n",
    "**Unity ML‑Agents Toolkit** is a bridge between **Unity** (where your simulation environment with Agents runs) and a **Python trainer** (PPO, SAC, Imitation Learning…) that trains neural networks based on **observations, actions, and rewards**. The training output is an **ONNX model** you can use directly in Unity (inference) without Python.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Key Concepts (Essential Theory)\n",
    "\n",
    "### Observations\n",
    "\n",
    "What the agent \"sees\". Can be **numeric** (vectors: distances, velocities, angles…), **raycasts**, or **visual** (images from cameras). Observations are **from the agent's perspective**, not the global scene state.\n",
    "\n",
    "### Actions\n",
    "\n",
    "What the agent can do. **Discrete** (a choice among N actions) or **continuous** (real values, e.g., acceleration/rotation).\n",
    "\n",
    "### Reward (Reward Signals)\n",
    "\n",
    "Scalars expressing how well the agent is performing.\n",
    "\n",
    "* **Extrinsic**: Defined by the environment (reach goal → +1, fall → −1…).\n",
    "* **Intrinsic** (optional): Generated by the trainer to guide exploration (e.g., **Curiosity**, **RND**) or for imitation (**GAIL**).\n",
    "\n",
    "### Policy\n",
    "\n",
    "The **neural network** that maps observations → actions.\n",
    "\n",
    "* **Training**: The policy is optimized in Python while the simulation runs in Unity.\n",
    "* **Inference**: The exported policy (ONNX) runs inside Unity at runtime.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Ecosystem Components\n",
    "\n",
    "* **Learning Environment (Unity)**: Your scene with **Agents**, physics, reward logic, and episode end rules.\n",
    "* **Agent (C#)**: Script defining **CollectObservations**, applying **Actions** in `OnActionReceived`, assigning **Reward**, and handling resets/terminations.\n",
    "* **Behavior**: Agent parameters (observation/action spaces, Behavior Name, type: Learning / Heuristic / Inference).\n",
    "* **Python Low‑Level API (mlagents_envs)**: Communication channel with Unity.\n",
    "* **External Communicator**: The \"cable\" connecting Unity ↔ Python.\n",
    "* **Python Trainers (mlagents)**: RL/IL algorithms (CLI `mlagents-learn`).\n",
    "* **Wrapper**: **Gym** and **PettingZoo** integrations for using Unity with other algorithms.\n",
    "* **Side Channels**: Custom data exchange (e.g., environment parameters, curriculum, randomization).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Training/Inference Modes\n",
    "\n",
    "* **Built‑in training**: Unity sends observations → Python calculates actions, optimizes policy; after training, export **ONNX** and use it in Unity.\n",
    "* **Cross‑platform inference**: ONNX runs on all platforms supported by Unity.\n",
    "* **Custom training**: You can control everything from Python using the low-level API or use Gym/PettingZoo wrappers.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Training Scenarios\n",
    "\n",
    "* Standard **Single‑Agent**.\n",
    "* **Simultaneous Single‑Agent**: Multiple copies of the same agent in parallel (same Behavior) for stability and speed.\n",
    "* **Adversarial Self‑Play**: Agents training against historical versions of each other (PPO recommended).\n",
    "* **Cooperative Multi‑Agent**: Shared rewards; **MA‑POCA** support (centralized credit assignment) even with self‑play.\n",
    "* **Competitive Multi‑Agent** or **Ecosystem**: Conflicting objectives, different species/roles.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Training Methods (Environment‑Agnostic)\n",
    "\n",
    "* **RL**:\n",
    "\n",
    "  * **PPO** (default): Robust, general‑purpose.\n",
    "  * **SAC**: Off‑policy, very sample‑efficient, ideal for continuous actions and slow environments; uses a replay buffer.\n",
    "* **Intrinsic rewards**: **Curiosity** and **RND** to guide exploration in sparse reward environments.\n",
    "* **Imitation Learning**:\n",
    "\n",
    "  * **BC** (Behavioral Cloning): Exact replication of demonstrations.\n",
    "  * **GAIL**: Adversarial reward for \"more similar to demos\"; often combined with extrinsic + BC.\n",
    "\n",
    "> You can combine RL + BC + GAIL (e.g., start from demos to unlock difficult environments, then fine‑tune with RL).\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Tools for Realistic Environments\n",
    "\n",
    "* **Curriculum Learning**: Gradually introduce difficulty (scene parameters that evolve with performance).\n",
    "* **Environment Parameter Randomization** (Domain Randomization): Randomly sample environment parameters to make the agent more **robust** and generalize better.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Supported Model Types\n",
    "\n",
    "* **Vector / Raycast**: MLP fully‑connected (configurable: hidden units, layers).\n",
    "* **Visual**: CNN (simple encoder, DQN‑style, **IMPALA ResNet**) with multiple cameras per agent.\n",
    "* **Variable‑length obs**: **Attention** for dynamic lists of entities.\n",
    "* **Memory**: **LSTM** for partial observability and context‑dependent decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Installation (Release 23: Recommended Setup)\n",
    "\n",
    "> Target: Unity **6000.0+** · Python **3.10.12** · Unity package `com.unity.ml-agents **4.0.0**` · Python packages `mlagents==1.1.0`, `mlagents-envs==1.1.0`.\n",
    "\n",
    "### 9.1 Unity\n",
    "\n",
    "1. Install **Unity 6000.x** via **Unity Hub**.\n",
    "2. Create a 3D project (e.g., *MLA_TestProject*).\n",
    "3. **Package Manager** → **Add package by name…** → `com.unity.ml-agents` (enable *Show preview packages* if needed).\n",
    "\n",
    "   * **Or (Development)**: Clone the repo and **Add package from disk…** pointing to `com.unity.ml-agents/package.json`.\n",
    "\n",
    "### 9.2 Python (Recommended with Conda)\n",
    "\n",
    "```bash\n",
    "# Create a dedicated environment\n",
    "conda create -n mlagents_py310 python=3.10.12 -y\n",
    "conda activate mlagents_py310\n",
    "\n",
    "# Install versions compatible with Release 23\n",
    "python -m pip install --upgrade pip\n",
    "pip install \"mlagents==1.1.0\" \"mlagents-envs==1.1.0\"\n",
    "# If grpcio fails on Windows:\n",
    "# conda install \"grpcio=1.48.2\" -c conda-forge\n",
    "\n",
    "# Verify\n",
    "mlagents-learn --help\n",
    "```\n",
    "\n",
    "> **Windows/GPU Note (Optional)**: If you want CUDA 12.1, install `torch~=2.2.1` from the NVIDIA PyTorch index first, then ML‑Agents. For CPU‑only, the ML‑Agents wheels come with a supported version of torch.\n",
    "\n",
    "### 9.3 (Dev Option) Install from Source\n",
    "\n",
    "```bash\n",
    "git clone --branch release_23 https://github.com/Unity-Technologies/ml-agents.git\n",
    "cd ml-agents\n",
    "python -m pip install ./ml-agents-envs\n",
    "python -m pip install ./ml-agents\n",
    "mlagents-learn --help\n",
    "```\n",
    "\n",
    "For live edits to Python packages:\n",
    "\n",
    "```bash\n",
    "pip install -e ./ml-agents-envs\n",
    "pip install -e ./ml-agents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Run the **3D Balance Ball** Sample (End‑to‑End)\n",
    "\n",
    "### 10.1 Open the Sample Scene\n",
    "\n",
    "* Open the `ml-agents/Project` project in Unity.\n",
    "* Scene: `Assets/ML-Agents/Examples/3DBall/Scenes/3DBall.unity`.\n",
    "* Each platform has an **Agent** with **Behavior Name = 3DBall**, **Obs size = 8**, **Continuous Actions = 2**.\n",
    "\n",
    "### 10.2 Try a **Pre‑trained Model** (Inference)\n",
    "\n",
    "1. Project → `Assets/ML-Agents/Examples/3DBall/TFModels/`.\n",
    "2. Select the Agent → **Behavior Parameters**:\n",
    "\n",
    "   * **Behavior Type**: *Inference Only*.\n",
    "   * **Model**: Drag and drop the pre‑trained `3DBall.onnx`.\n",
    "   * **Inference Device**: CPU.\n",
    "3. **Play**: The platforms balance the ball.\n",
    "\n",
    "### 10.3 **Training from Scratch** (PPO/SAC)\n",
    "\n",
    "1. In the terminal (active env):\n",
    "\n",
    "   ```bash\n",
    "   # PPO recommended to converge quickly on 3DBall\n",
    "   mlagents-learn config/ppo/3DBall.yaml --run-id=bb_ppo_01\n",
    "   # Or SAC\n",
    "   # mlagents-learn config/sac/3DBall.yaml --run-id=bb_sac_01\n",
    "   ```\n",
    "2. When \"**Listening on port …**\" appears, go back to Unity.\n",
    "3. **Behavior Type**: *Default* (training). **Model**: Empty (no ONNX assigned).\n",
    "4. Press **Play**. In the terminal, you'll see **Mean Reward** increasing.\n",
    "\n",
    "### 10.4 Monitor with **TensorBoard** (Optional)\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir results\n",
    "```\n",
    "\n",
    "Open `http://localhost:6006` → follow *Environment/Cumulative Reward*.\n",
    "\n",
    "### 10.5 Export and Use the **ONNX Model**\n",
    "\n",
    "* Stop with **Ctrl+C**: The trainer saves the model in `results/<run-id>/.../Policy.onnx` (sometimes also copies `<behavior>.onnx` at the run level).\n",
    "* Copy the file to `Assets/ML-Agents/Examples/3DBall/TFModels/`.\n",
    "* In **Behavior Parameters** set:\n",
    "\n",
    "  * **Behavior Type**: *Inference Only*.\n",
    "  * **Model**: Your `.onnx`.\n",
    "  * **Play** to test the behavior **trained by you**.\n",
    "\n",
    "> **Resume**: To continue an interrupted run, rerun the same command with `--resume`. If the run‑id already exists and you want to start over, use `--force` or change `--run-id`.\n",
    "\n",
    "---\n",
    "\n",
    "## 11) What Defines the **Task** vs. the **Training**\n",
    "\n",
    "* **Task** (what to learn): Is in the **C# code** of the Agent and the scene (observations, actions, rewards, reset).\n",
    "* **Training** (how to learn): Is in the **YAML file** (algorithm, hyperparameters, max_steps, summary_freq, reward signals, etc.).\n",
    "* **Output**: The trained **policy** in **ONNX** format (inference only). To re‑train, you need the **checkpoints** (not the ONNX).\n",
    "\n",
    "---\n",
    "\n",
    "## 12) Tips & Pitfalls\n",
    "\n",
    "* **Behavior Name** in Unity must match the one in the YAML file.\n",
    "* During **training**, leave **Model** empty (no ONNX assigned) and **Behavior Type = Default**.\n",
    "* If `mlagents-learn` says the run‑id exists, use `--resume`, `--force`, or a new `--run-id`.\n",
    "* On Windows, if `grpcio` fails in build: `conda install \"grpcio=1.48.2\" -c conda-forge` and reinstall the packages.\n",
    "* For complex environments (sparse rewards), consider **Curiosity**/**RND** and/or **BC/GAIL** with demos.\n",
    "\n",
    "---\n",
    "\n",
    "## 13) Quick Glossary\n",
    "\n",
    "* **Agent**: GameObject with a script that observes, acts, and receives rewards.\n",
    "* **Behavior**: Agent's decision parameters (spaces, name, type).\n",
    "* **Policy**: Neural network mapping observations → actions.\n",
    "* **Trainer**: Python process optimizing the policy (PPO/SAC/GAIL/BC).\n",
    "* **ONNX**: Exported model for inference in Unity.\n",
    "* **Checkpoint**: Trainable state of the trainer for `--resume`.\n",
    "* **Curriculum / Randomization**: Techniques for progressive difficulty and robustness."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
