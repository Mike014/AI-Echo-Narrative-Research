{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e16205f",
   "metadata": {},
   "source": [
    "# Working Directory (First Step)\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/recon/mike014 && cd ~/recon/mike014\n",
    "```\n",
    "What it does: Creates the folder to save all results and navigates into it.\n",
    "\n",
    "---\n",
    "\n",
    "## A ‚Äî Passive OSINT (No Server Contact)\n",
    "\n",
    "1. Download page + HTTP headers\n",
    "\n",
    "```bash\n",
    "curl -sSL -D headers.txt -o page.html https://mike014.github.io/michele-portfolio/resume.html\n",
    "```\n",
    "What it does: Saves HTTP headers in `headers.txt` and HTML source in `page.html`.\n",
    "\n",
    "2. Extract all links from the page\n",
    "\n",
    "```bash\n",
    "grep -Eo \"(http|https)://[a-zA-Z0-9./?=_-]*\" page.html | sort -u > links.txt\n",
    "```\n",
    "What it does: Finds and saves all unique URLs found in the HTML to `links.txt`.\n",
    "\n",
    "3. Extract emails from the source\n",
    "\n",
    "```bash\n",
    "grep -Eio \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\" page.html | sort -u > emails.txt\n",
    "```\n",
    "What it does: Finds and saves any exposed email addresses from the page.\n",
    "\n",
    "4. Check robots.txt and sitemap (if they exist)\n",
    "\n",
    "```bash\n",
    "curl -sS https://mike014.github.io/robots.txt -o robots.txt || echo \"no robots\"\n",
    "curl -sS https://mike014.github.io/sitemap.xml -o sitemap.xml || echo \"no sitemap\"\n",
    "```\n",
    "What it does: Saves `robots.txt` and `sitemap.xml` (or writes \"no robots/no sitemap\" if they don't exist).\n",
    "\n",
    "5. Search public certificates with crt.sh (passive)\n",
    "\n",
    "```bash\n",
    "curl -s \"https://crt.sh/?q=%25.mike014.github.io\" | grep -Eo \"mike014.github.io|[A-Za-z0-9.-]+\\.mike014\\.github\\.io\" | sort -u\n",
    "```\n",
    "What it does: Queries crt.sh and filters any records/certificates containing your domain or subdomains.\n",
    "\n",
    "---\n",
    "\n",
    "## B ‚Äî Passive Collection with Tools\n",
    "\n",
    "1. theHarvester (OSINT)\n",
    "\n",
    "```bash\n",
    "theharvester -d mike014.github.io -b all -l 500 -f theharvester.html\n",
    "```\n",
    "What it does: Collects emails, subdomains, and public information from multiple sources; saves an HTML report `theharvester.html`.\n",
    "\n",
    "2. Limited alternative (Google only)\n",
    "\n",
    "```bash\n",
    "theharvester -d mike014.github.io -b google -l 200 -f th.html\n",
    "```\n",
    "What it does: Same as above but only via Google, less noise.\n",
    "\n",
    "---\n",
    "\n",
    "## C ‚Äî Active (Light Scanning ‚Äî OK Since It's Your Site)\n",
    "\n",
    "> Use these only on assets you own (as in this case).\n",
    "\n",
    "1. TLS Certificate Info\n",
    "\n",
    "```bash\n",
    "openssl s_client -connect mike014.github.io:443 -servername mike014.github.io </dev/null 2>/dev/null | openssl x509 -noout -text > cert_info.txt\n",
    "```\n",
    "What it does: Opens a TLS connection, extracts the certificate and saves it to `cert_info.txt` (issuer, expiration, SAN).\n",
    "\n",
    "2. Light Web Scan (Ports 80 and 443 + Banner)\n",
    "\n",
    "```bash\n",
    "nmap -Pn -p 80,443 --script=http-title,http-server-header -oN nmap_http.txt mike014.github.io\n",
    "```\n",
    "What it does: Performs a targeted scan on 80/443, tries to read the HTTP title and server header; saves output to `nmap_http.txt`.\n",
    "\n",
    "3. Deep Scan (Noisy ‚Äî Use with Caution)\n",
    "\n",
    "```bash\n",
    "nmap -sV -A -p 1-65535 --min-rate=1000 -oN nmap_full.txt mike014.github.io\n",
    "```\n",
    "What it does: Full scan of all ports, service detection, OS detection and `-A` scripts. Very intrusive; not necessary for GitHub Pages.\n",
    "\n",
    "---\n",
    "\n",
    "## D ‚Äî Static Resource Enumeration / Download\n",
    "\n",
    "1. Download Site (Limited Mirror)\n",
    "\n",
    "```bash\n",
    "wget --mirror --level=1 --no-parent -e robots=off https://mike014.github.io/michele-portfolio/\n",
    "```\n",
    "What it does: Recursively downloads the page and linked resources up to depth 1.\n",
    "\n",
    "2. Directory Brute-Force (Only If You Have Permission)\n",
    "\n",
    "```bash\n",
    "gobuster dir -u https://mike014.github.io/michele-portfolio/ -w /usr/share/wordlists/dirb/common.txt -t 20\n",
    "```\n",
    "What it does: Attempts to discover hidden paths using a wordlist; use cautiously and limit the rate.\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Script (Passive ‚Üí Light Active)\n",
    "\n",
    "Save to `~/recon/recon_mike014.sh`, make it executable and run it:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "TARGET=\"mike014.github.io/michele-portfolio/resume.html\"\n",
    "HOST=\"mike014.github.io\"\n",
    "OUTDIR=\"$HOME/recon/mike014\"\n",
    "mkdir -p \"$OUTDIR\"\n",
    "cd \"$OUTDIR\" || exit 1\n",
    "\n",
    "curl -sSL -D headers.txt -o page.html \"https://$TARGET\"\n",
    "\n",
    "grep -Eo \"(http|https)://[A-Za-z0-9./?=_&%-]*\" page.html | sort -u > links.txt || true\n",
    "grep -Eio \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\" page.html | sort -u > emails.txt || true\n",
    "\n",
    "curl -sS \"https://$HOST/robots.txt\" -o robots.txt || echo \"no robots\" > robots.txt\n",
    "curl -sS \"https://$HOST/sitemap.xml\" -o sitemap.xml || echo \"no sitemap\" > sitemap.xml\n",
    "\n",
    "openssl s_client -connect $HOST:443 -servername $HOST </dev/null 2>/dev/null | openssl x509 -noout -text > cert_info.txt || echo \"cert fetch failed\" > cert_info.txt\n",
    "\n",
    "nmap -Pn -p 80,443 --script=http-title,http-server-header -oN nmap_http.txt $HOST\n",
    "\n",
    "ls -l \"$OUTDIR\"\n",
    "```\n",
    "What it does: Automates all essential commands and saves outputs to `~/recon/mike014/`.\n",
    "\n",
    "Execution:\n",
    "\n",
    "```bash\n",
    "chmod +x ~/recon/recon_mike014.sh\n",
    "~/recon/recon_mike014.sh\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîç What to Look for in the Output Files\n",
    "\n",
    "* `headers.txt` ‚Üí Look for `Server:`, `X-Cache`, security headers (CSP, HSTS).\n",
    "* `page.html` / `links.txt` ‚Üí Check external resources, CDNs, analytics.\n",
    "* `emails.txt` ‚Üí Any exposed contacts.\n",
    "* `cert_info.txt` ‚Üí Issuer (Let's Encrypt or GitHub), SAN (subdomains).\n",
    "* `nmap_http.txt` ‚Üí Ports and banners (for GitHub Pages typically only 443).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ef325",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
